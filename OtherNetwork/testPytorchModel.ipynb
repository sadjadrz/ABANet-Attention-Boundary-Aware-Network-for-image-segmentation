{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P_mjrQeLkdMi"
   },
   "source": [
    "###import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "kpI03b_NfXgF"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "#os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "import numpy as np\n",
    "\n",
    "#import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "import albumentations as albu\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset as BaseDataset\n",
    "import cv2\n",
    "from sklearn.metrics import accuracy_score , f1_score , jaccard_score ,precision_score , recall_score\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import segmentation_models_pytorch as smp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AA0LE0S-kzdg"
   },
   "source": [
    "## Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "LenVic9tVU3W"
   },
   "outputs": [],
   "source": [
    "class Dataset(BaseDataset):\n",
    "    \"\"\"CamVid Dataset. Read images, apply augmentation and preprocessing transformations.\n",
    "    \n",
    "    Args:\n",
    "        images_dir (str): path to images folder\n",
    "        masks_dir (str): path to segmentation masks folder\n",
    "        class_values (list): values of classes to extract from segmentation mask\n",
    "        augmentation (albumentations.Compose): data transfromation pipeline \n",
    "            (e.g. flip, scale, etc.)\n",
    "        preprocessing (albumentations.Compose): data preprocessing \n",
    "            (e.g. noralization, shape manipulation, etc.)\n",
    "    \n",
    "    \n",
    "    CLASSES = ['sky', 'building', 'pole', 'road', 'pavement', \n",
    "               'tree', 'signsymbol', 'fence', 'car', \n",
    "               'pedestrian', 'bicyclist', 'unlabelled']\n",
    "    \"\"\"\n",
    "    CLASSES = ['masks']\n",
    "    \n",
    "    def __init__(\n",
    "            self, \n",
    "            images_dir, \n",
    "            masks_dir, \n",
    "            classes=None, \n",
    "            augmentation=None, \n",
    "            preprocessing=None,\n",
    "    ):\n",
    "        self.ids = os.listdir(images_dir)\n",
    "        self.images_fps = [os.path.join(images_dir, image_id) for image_id in self.ids]\n",
    "        self.masks_fps = [os.path.join(masks_dir, image_id) for image_id in self.ids]\n",
    "        \n",
    "        # convert str names to class values on masks\n",
    "        self.class_values = [self.CLASSES.index(cls.lower()) for cls in classes]\n",
    "        \n",
    "        self.augmentation = augmentation\n",
    "        self.preprocessing = preprocessing\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        \n",
    "        # read data\n",
    "        image = cv2.imread(self.images_fps[i])\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        image  = cv2.resize(image,(256,256))\n",
    "        mask = cv2.imread(self.masks_fps[i], 0)\n",
    "        mask  = cv2.resize(mask,(256,256))\n",
    "        \n",
    "        # extract certain classes from mask (e.g. cars)\n",
    "        masks = [(mask == v) for v in self.class_values]\n",
    "        mask = np.stack(masks, axis=-1).astype('float')\n",
    "        \n",
    "        # apply augmentations\n",
    "        if self.augmentation:\n",
    "            sample = self.augmentation(image=image, mask=mask)\n",
    "            image, mask = sample['image'], sample['mask']\n",
    "        \n",
    "        # apply preprocessing\n",
    "        if self.preprocessing:\n",
    "            sample = self.preprocessing(image=image, mask=mask)\n",
    "            image, mask = sample['image'], sample['mask']\n",
    "            \n",
    "        return image, mask\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "VGCAV4nMhAw6"
   },
   "outputs": [],
   "source": [
    "\n",
    "#import zipfile\n",
    "#with zipfile.ZipFile('/content/drive/MyDrive/mask_segmentation/dataset/DatasetV4.zip', 'r') as zip_ref:\n",
    " #   zip_ref.extractall('/content/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "LiQGTFuZkyn4"
   },
   "outputs": [],
   "source": [
    "DATA_DIR = 'E:\\drive D\\python code\\Boundary-Aware Segmentation Network for Mobile and Web Applications\\code\\BASNet\\OtherNetwork\\Dataset_Test\\\\'\n",
    "\n",
    "x_test_dir = os.path.join(DATA_DIR, 'image/')\n",
    "y_test_dir = os.path.join(DATA_DIR, 'mask/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "64wzej_DlHL_"
   },
   "source": [
    "### Dataloader\n",
    "\n",
    "Writing helper class for data extraction, tranformation and preprocessing  \n",
    "https://pytorch.org/docs/stable/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "7RTDcWz-lARP"
   },
   "outputs": [],
   "source": [
    "class Dataset(BaseDataset):\n",
    "    \"\"\"CamVid Dataset. Read images, apply augmentation and preprocessing transformations.\n",
    "    \n",
    "    Args:\n",
    "        images_dir (str): path to images folder\n",
    "        masks_dir (str): path to segmentation masks folder\n",
    "        class_values (list): values of classes to extract from segmentation mask\n",
    "        augmentation (albumentations.Compose): data transfromation pipeline \n",
    "            (e.g. flip, scale, etc.)\n",
    "        preprocessing (albumentations.Compose): data preprocessing \n",
    "            (e.g. noralization, shape manipulation, etc.)\n",
    "    \n",
    "    \n",
    "    CLASSES = ['sky', 'building', 'pole', 'road', 'pavement', \n",
    "               'tree', 'signsymbol', 'fence', 'car', \n",
    "               'pedestrian', 'bicyclist', 'unlabelled']\n",
    "    \"\"\"\n",
    "    CLASSES = ['masks']\n",
    "    \n",
    "    def __init__(\n",
    "            self, \n",
    "            images_dir, \n",
    "            masks_dir, \n",
    "            classes=None, \n",
    "            augmentation=None, \n",
    "            preprocessing=None,\n",
    "    ):\n",
    "        self.ids = os.listdir(images_dir)\n",
    "        self.images_fps = [os.path.join(images_dir, image_id) for image_id in self.ids]\n",
    "        self.masks_fps = [os.path.join(masks_dir, image_id) for image_id in self.ids]\n",
    "        \n",
    "        # convert str names to class values on masks\n",
    "        self.class_values = [self.CLASSES.index(cls.lower()) for cls in classes]\n",
    "        \n",
    "        self.augmentation = augmentation\n",
    "        self.preprocessing = preprocessing\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        \n",
    "        # read data\n",
    "        image = cv2.imread(self.images_fps[i])\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        image  = cv2.resize(image,(256,256))\n",
    "        mask = cv2.imread(self.masks_fps[i], 0)\n",
    "        mask  = cv2.resize(mask,(256,256))\n",
    "        \n",
    "        # extract certain classes from mask (e.g. cars)\n",
    "        masks = [(mask == v) for v in self.class_values]\n",
    "        mask = np.stack(masks, axis=-1).astype('float')\n",
    "        \n",
    "        # apply augmentations\n",
    "        if self.augmentation:\n",
    "            sample = self.augmentation(image=image, mask=mask)\n",
    "            image, mask = sample['image'], sample['mask']\n",
    "        \n",
    "        # apply preprocessing\n",
    "        if self.preprocessing:\n",
    "            sample = self.preprocessing(image=image, mask=mask)\n",
    "            image, mask = sample['image'], sample['mask']\n",
    "            \n",
    "        return image, mask ,self.images_fps[i]\n",
    "        #return self.images_fps[i]  \n",
    "    def __len__(self):\n",
    "        return len(self.ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z15qTegNlhuH"
   },
   "source": [
    "### Augmentations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "id": "2T3VScOplRRt"
   },
   "outputs": [],
   "source": [
    "ENCODER = 'efficientnet-b7'\n",
    "ENCODER_WEIGHTS = 'imagenet'\n",
    "preprocessing_fn = smp.encoders.get_preprocessing_fn(ENCODER, ENCODER_WEIGHTS)\n",
    "# Dice/F1 score - https://en.wikipedia.org/wiki/S%C3%B8rensen%E2%80%93Dice_coefficient\n",
    "# IoU/Jaccard score - https://en.wikipedia.org/wiki/Jaccard_index\n",
    "\n",
    "loss = smp.utils.losses.DiceLoss()\n",
    "metrics = [smp.utils.metrics.Precision(),]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_training_augmentation():\n",
    "    train_transform = [\n",
    "\n",
    "        albu.HorizontalFlip(p=0.5),\n",
    "\n",
    "        #albu.ShiftScaleRotate(scale_limit=0.5, rotate_limit=0, shift_limit=0.1, p=1, border_mode=0),\n",
    "\n",
    "        #albu.PadIfNeeded(min_height=320, min_width=320, always_apply=True, border_mode=0),\n",
    "        #albu.RandomCrop(height=320, width=320, always_apply=True),\n",
    "\n",
    "        albu.GaussNoise(p=0.2),\n",
    "        #albu.IAAPerspective(p=0.5),\n",
    "\n",
    "        albu.OneOf(\n",
    "            [\n",
    "                albu.CLAHE(p=0.5),\n",
    "                albu.RandomBrightness(p=1),\n",
    "                albu.RandomGamma(p=0.4),\n",
    "            ],\n",
    "            p=0.9,\n",
    "        ),\n",
    "\n",
    "        albu.OneOf(\n",
    "            [\n",
    "                albu.Sharpen(p=1),\n",
    "                albu.Blur(blur_limit=3, p=1),\n",
    "                albu.MotionBlur(blur_limit=3, p=1),\n",
    "            ],\n",
    "            p=0.9,\n",
    "        ),\n",
    "\n",
    "        albu.OneOf(\n",
    "            [\n",
    "                albu.RandomContrast(p=1),\n",
    "                albu.HueSaturationValue(p=1),\n",
    "            ],\n",
    "            p=0.9,\n",
    "        ),\n",
    "    ]\n",
    "    return albu.Compose(train_transform)\n",
    "\n",
    "\n",
    "def get_validation_augmentation():\n",
    "    \"\"\"Add paddings to make image shape divisible by 32\"\"\"\n",
    "    test_transform = [\n",
    "        albu.PadIfNeeded(256, 256)\n",
    "    ]\n",
    "    return albu.Compose(test_transform)\n",
    "\n",
    "\n",
    "def to_tensor(x, **kwargs):\n",
    "    return x.transpose(2, 0, 1).astype('float32')\n",
    "\n",
    "\n",
    "def get_preprocessing(preprocessing_fn):\n",
    "    \"\"\"Construct preprocessing transform\n",
    "    \n",
    "    Args:\n",
    "        preprocessing_fn (callbale): data normalization function \n",
    "            (can be specific for each pretrained neural network)\n",
    "    Return:\n",
    "        transform: albumentations.Compose\n",
    "    \n",
    "    \"\"\"\n",
    "    print(albu.__version__)\n",
    "    _transform = [\n",
    "        albu.Lambda(image=preprocessing_fn),\n",
    "        albu.Lambda(image=to_tensor, mask=to_tensor),\n",
    "    ]\n",
    "    return albu.Compose(_transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8L-Ev4TSmOMu"
   },
   "source": [
    "## Test best saved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "id": "EYalK3_cl-SE"
   },
   "outputs": [],
   "source": [
    "#best_model = torch.load('D:\\python code\\paper code\\model\\model\\Morphological\\weight\\\\deeplabv3++\\\\best_model-22-.pth',map_location=torch.device('cuda'))\n",
    "best_model = torch.load('E:\\drive D\\\\Net\\\\Unet++\\\\efficient-b7\\\\best_model_efficientnet-b7_29-.pth',map_location=torch.device('cuda'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BBk0SrErmTlM",
    "outputId": "c98959d7-5dc7-4367-983b-e7e39b151651"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0.3\n"
     ]
    }
   ],
   "source": [
    "# create test dataset\n",
    "CLASSES = ['masks']\n",
    "\n",
    "test_dataset = Dataset(\n",
    "    x_test_dir, \n",
    "    y_test_dir, \n",
    "    augmentation=get_validation_augmentation(), \n",
    "    preprocessing=get_preprocessing(preprocessing_fn),\n",
    "    classes=CLASSES,\n",
    ")\n",
    "\n",
    "test_dataloader = DataLoader(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "outputs": [],
   "source": [
    "# # evaluate model on test set\n",
    "# test_epoch = smp.utils.train.ValidEpoch(\n",
    "#     model=best_model,\n",
    "#     loss=smp.utils.losses.DiceLoss(),\n",
    "#     metrics=metrics,\n",
    "#     device='cuda',\n",
    "# )\n",
    "# logs = test_epoch.run(test_dataloader)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "outputs": [],
   "source": [
    "def apply_dilated(img,kernelsize ,iteration):\n",
    "    kernel = np.ones(kernelsize,np.uint8)\n",
    "    result_img = cv2.dilate(img,kernel ,iterations=iteration)\n",
    "    return result_img\n",
    "\n",
    "def apply_morph_opening(img,kernelSize):\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_RECT, kernelSize)\n",
    "    opening = cv2.morphologyEx(img, cv2.MORPH_OPEN, kernel)\n",
    "    return opening\n",
    "\n",
    "def apply_erode(img,kernelsize ,iteration):\n",
    "    kernel = np.ones(kernelsize,np.uint8)\n",
    "    result_img = cv2.erode(img.copy(),kernel ,iterations=iteration)\n",
    "    return result_img\n",
    "\n",
    "def apply_morph_closing(img,kernelSize):\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_RECT, kernelSize)\n",
    "    closing = cv2.morphologyEx(img, cv2.MORPH_CLOSE, kernel)\n",
    "    return closing\n",
    "\n",
    "def write_image(dir_net,pr_mask):\n",
    "    cv2.imwrite('E:\\drive D\\\\Net\\\\Unet++\\\\efficient-b7\\\\'+dir_net+'\\\\'+filename,pr_mask)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "274"
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "ny6nTLwUmzmz",
    "outputId": "291b72b7-69fe-40c5-b642-2ae834923535"
   },
   "outputs": [],
   "source": [
    "DEVICE = 'cuda'\n",
    "for i in range(len(test_dataset)):\n",
    "    #n = np.random.choice(len(test_dataset))\n",
    "    \n",
    "    #image_vis = test_dataset_vis[n][0].astype('uint8')\n",
    "    image, gt_mask,name = test_dataset[i]\n",
    "    \n",
    "    gt_mask = gt_mask.squeeze()\n",
    "    \n",
    "    x_tensor = torch.from_numpy(image).to(DEVICE).unsqueeze(0)\n",
    "    pr_mask = best_model.predict(x_tensor)\n",
    "    pr_mask = (pr_mask.squeeze().cpu().numpy().round())\n",
    "    pr_mask = pr_mask.reshape(256,256,1)\n",
    "    pr_mask = pr_mask[:,:,0]\n",
    "    pr_mask = cv2.resize(pr_mask,(256,256))\n",
    "    pr_mask *=255\n",
    "    ret , pr_mask = cv2.threshold(pr_mask, 120, 255, cv2.THRESH_BINARY_INV)\n",
    "\n",
    "    # pr_mask_erode = apply_erode(pr_mask,(3,3),2)\n",
    "    # pr_mask_dilated = apply_dilated(pr_mask , (3,3),2)\n",
    "    #\n",
    "    # pr_mask_dilated1 = apply_dilated(pr_mask , (7,7),1)\n",
    "    # pr_mask_dilated_erode = apply_erode(pr_mask_dilated1,(3,3),4)\n",
    "    # pr_mask_dilated_erode1 = apply_erode(pr_mask_dilated1,(3,3),5)\n",
    "    filename = os.path.basename(os.path.normpath(name))\n",
    "    write_image('result',pr_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_f1score_cal(ground_truth_path , predicted_path,csvName):\n",
    "    ground_truth_label = sorted(glob(ground_truth_path)) \n",
    "    predict_label = sorted(glob(predicted_path))\n",
    "\n",
    "    SCORE=[]\n",
    "    for truth,predict in tqdm(zip(ground_truth_label,predict_label) , total=len(ground_truth_label)):\n",
    "        image_name = truth.split(\"/\")[-1]\n",
    "        truth = cv2.imread(truth)\n",
    "        truth = cv2.resize(truth,(256,256))\n",
    "        truth = truth/255\n",
    "        truth = truth.astype(np.int32)          \n",
    "        #cv2.imshow('d',truth)\n",
    "        #cv2.waitKey(0)\n",
    "        predict = cv2.imread(predict)\n",
    "        predict = predict/255 \n",
    "        predict = predict.astype(np.int32)  \n",
    "        truth = truth.flatten()\n",
    "        predict = predict.flatten()\n",
    "\n",
    "        acc_value = accuracy_score(truth,predict)\n",
    "        f1 = f1_score(truth,predict , labels=[0,1],average=\"binary\")\n",
    "        jaccard = jaccard_score(truth,predict , labels=[0,1],average=\"binary\")\n",
    "        preceision = precision_score(truth,predict , labels=[0,1],average=\"binary\")\n",
    "        recall = recall_score(truth,predict , labels=[0,1],average=\"binary\")\n",
    "\n",
    "        SCORE.append([image_name , acc_value,f1 ,jaccard , preceision ,recall])\n",
    "    score = [ s[1:] for s in SCORE]\n",
    "    score = np.mean(score , axis=0)\n",
    "    df = pd.DataFrame(SCORE , columns=[\"imagename\" , \"accuracy\" , \"F1\" , \"jaccard\"  ,\"preceision\" , \"recall\"])\n",
    "\n",
    "    df.to_csv(csvName+\".csv\")\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 274/274 [00:55<00:00,  4.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy_result: 0.95889\n",
      "f1_Score_result: 0.93960\n",
      "IoU:result 0.88912\n",
      "pre:result 0.91190\n",
      "recall:result 0.97234\n",
      "///////////////////////////////////////////////////////////////\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 274/274 [00:55<00:00,  4.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy_result: 0.81284\n",
      "f1_Score_result: 0.73779\n",
      "IoU:result 0.60151\n",
      "pre:result 0.71406\n",
      "recall:result 0.80654\n",
      "///////////////////////////////////////////////////////////////\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 274/274 [00:54<00:00,  5.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy_result: 0.95999\n",
      "f1_Score_result: 0.94212\n",
      "IoU:result 0.89242\n",
      "pre:result 0.91450\n",
      "recall:result 0.97418\n",
      "///////////////////////////////////////////////////////////////\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 274/274 [00:54<00:00,  5.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy_result: 0.96083\n",
      "f1_Score_result: 0.94320\n",
      "IoU:result 0.89463\n",
      "pre:result 0.91460\n",
      "recall:result 0.97665\n",
      "///////////////////////////////////////////////////////////////\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 274/274 [00:53<00:00,  5.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy_result: 0.77806\n",
      "f1_Score_result: 0.69914\n",
      "IoU:result 0.55394\n",
      "pre:result 0.67325\n",
      "recall:result 0.76248\n",
      "///////////////////////////////////////////////////////////////\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 274/274 [00:53<00:00,  5.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy_result: 0.95996\n",
      "f1_Score_result: 0.94198\n",
      "IoU:result 0.89187\n",
      "pre:result 0.91148\n",
      "recall:result 0.97645\n",
      "///////////////////////////////////////////////////////////////\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 274/274 [00:54<00:00,  5.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy_result: 0.96043\n",
      "f1_Score_result: 0.94250\n",
      "IoU:result 0.89335\n",
      "pre:result 0.91493\n",
      "recall:result 0.97490\n",
      "///////////////////////////////////////////////////////////////\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 274/274 [00:53<00:00,  5.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy_result: 0.96113\n",
      "f1_Score_result: 0.94320\n",
      "IoU:result 0.89484\n",
      "pre:result 0.91629\n",
      "recall:result 0.97482\n",
      "///////////////////////////////////////////////////////////////\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 274/274 [00:54<00:00,  5.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy_result: 0.95904\n",
      "f1_Score_result: 0.94040\n",
      "IoU:result 0.89011\n",
      "pre:result 0.91427\n",
      "recall:result 0.97152\n",
      "///////////////////////////////////////////////////////////////\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 274/274 [00:53<00:00,  5.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy_result: 0.95905\n",
      "f1_Score_result: 0.94082\n",
      "IoU:result 0.89064\n",
      "pre:result 0.91652\n",
      "recall:result 0.96978\n",
      "///////////////////////////////////////////////////////////////\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 274/274 [00:53<00:00,  5.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy_result: 0.96159\n",
      "f1_Score_result: 0.94373\n",
      "IoU:result 0.89587\n",
      "pre:result 0.91667\n",
      "recall:result 0.97529\n",
      "///////////////////////////////////////////////////////////////\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 274/274 [00:53<00:00,  5.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy_result: 0.95934\n",
      "f1_Score_result: 0.94078\n",
      "IoU:result 0.89061\n",
      "pre:result 0.91218\n",
      "recall:result 0.97405\n",
      "///////////////////////////////////////////////////////////////\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 274/274 [00:53<00:00,  5.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy_result: 0.95999\n",
      "f1_Score_result: 0.94212\n",
      "IoU:result 0.89242\n",
      "pre:result 0.91450\n",
      "recall:result 0.97418\n",
      "///////////////////////////////////////////////////////////////\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 274/274 [00:53<00:00,  5.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy_result: 0.96033\n",
      "f1_Score_result: 0.94241\n",
      "IoU:result 0.89312\n",
      "pre:result 0.91522\n",
      "recall:result 0.97435\n",
      "///////////////////////////////////////////////////////////////\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 274/274 [00:54<00:00,  5.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy_result: 0.96107\n",
      "f1_Score_result: 0.94298\n",
      "IoU:result 0.89466\n",
      "pre:result 0.91521\n",
      "recall:result 0.97590\n",
      "///////////////////////////////////////////////////////////////\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 274/274 [00:54<00:00,  4.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy_result: 0.95949\n",
      "f1_Score_result: 0.94029\n",
      "IoU:result 0.89085\n",
      "pre:result 0.91399\n",
      "recall:result 0.97196\n",
      "///////////////////////////////////////////////////////////////\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 274/274 [00:52<00:00,  5.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy_result: 0.95934\n",
      "f1_Score_result: 0.94078\n",
      "IoU:result 0.89061\n",
      "pre:result 0.91218\n",
      "recall:result 0.97405\n",
      "///////////////////////////////////////////////////////////////\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 274/274 [00:54<00:00,  5.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy_result: 0.95875\n",
      "f1_Score_result: 0.93934\n",
      "IoU:result 0.88844\n",
      "pre:result 0.91090\n",
      "recall:result 0.97352\n",
      "///////////////////////////////////////////////////////////////\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 274/274 [00:53<00:00,  5.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy_result: 0.96118\n",
      "f1_Score_result: 0.94321\n",
      "IoU:result 0.89516\n",
      "pre:result 0.91507\n",
      "recall:result 0.97630\n",
      "///////////////////////////////////////////////////////////////\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 274/274 [00:57<00:00,  4.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy_result: 0.96032\n",
      "f1_Score_result: 0.94120\n",
      "IoU:result 0.89188\n",
      "pre:result 0.91463\n",
      "recall:result 0.97318\n",
      "///////////////////////////////////////////////////////////////\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 274/274 [00:55<00:00,  4.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy_result: 0.96012\n",
      "f1_Score_result: 0.94175\n",
      "IoU:result 0.89208\n",
      "pre:result 0.91280\n",
      "recall:result 0.97533\n",
      "///////////////////////////////////////////////////////////////\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 274/274 [00:52<00:00,  5.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy_result: 0.96129\n",
      "f1_Score_result: 0.94336\n",
      "IoU:result 0.89524\n",
      "pre:result 0.91565\n",
      "recall:result 0.97567\n",
      "///////////////////////////////////////////////////////////////\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 274/274 [00:53<00:00,  5.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy_result: 0.95678\n",
      "f1_Score_result: 0.93750\n",
      "IoU:result 0.88526\n",
      "pre:result 0.90952\n",
      "recall:result 0.97106\n",
      "///////////////////////////////////////////////////////////////\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 274/274 [00:53<00:00,  5.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy_result: 0.95998\n",
      "f1_Score_result: 0.94195\n",
      "IoU:result 0.89242\n",
      "pre:result 0.91602\n",
      "recall:result 0.97225\n",
      "///////////////////////////////////////////////////////////////\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "nets = [\"Deep\", \"Deep++\",\"FPN\", \"Linknet\",\"Manet\",\"Pan\",\"Psp\",\"Unet\",\"Unet++\"]\n",
    "backBone = [\"efficient-b7\" , \"resnet50\" , \"vgg19\"]\n",
    "rootDir = \"E:\\drive D\\\\Net\\\\\"\n",
    "for i in nets:\n",
    "    for ii in backBone:\n",
    "        resultDir = os.path.join(rootDir,i,ii,\"result\")\n",
    "        if len(os.listdir(resultDir)) > 0:\n",
    "            acc_erode = accuracy_f1score_cal(\"Dataset_Test/mask/*\",resultDir+\"/*\",csvName= i+\"_\"+ii)\n",
    "            print(f\"accuracy_result: {acc_erode[0]:0.5f}\")\n",
    "            print(f\"f1_Score_result: {acc_erode[1]:0.5f}\")\n",
    "            print(f\"IoU:result {acc_erode[2]:0.5f}\")\n",
    "            print(f\"pre:result {acc_erode[3]:0.5f}\")\n",
    "            print(f\"recall:result {acc_erode[4]:0.5f}\")\n",
    "            print(\"///////////////////////////////////////////////////////////////\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 274/274 [00:53<00:00,  5.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy_result: 0.96129\n",
      "f1_Score_result: 0.94336\n",
      "IoU:result 0.89524\n",
      "pre:result 0.91565\n",
      "recall:result 0.97567\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "acc_erode = accuracy_f1score_cal(\"Dataset_Test/mask/*\",\"E:\\drive D\\\\Net\\\\Unet++\\\\efficient-b7\\\\result/*\" )\n",
    "print(f\"accuracy_result: {acc_erode[0]:0.5f}\")\n",
    "print(f\"f1_Score_result: {acc_erode[1]:0.5f}\")\n",
    "print(f\"IoU:result {acc_erode[2]:0.5f}\")\n",
    "print(f\"pre:result {acc_erode[3]:0.5f}\")\n",
    "print(f\"recall:result {acc_erode[4]:0.5f}\")\n",
    "print(\"///////////////////////////////////////////////////////////////\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n",
      "C:\\Users\\sadjad\\anaconda3\\envs\\mytfenv\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3441: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "C:\\Users\\sadjad\\anaconda3\\envs\\mytfenv\\lib\\site-packages\\numpy\\core\\_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "invalid index to scalar variable.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mIndexError\u001B[0m                                Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp/ipykernel_17444/3862019426.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[0mNets\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;34m\"Deep\"\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m\"Deep++\"\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;34m\"FPN\"\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      2\u001B[0m \u001B[0macc_erode\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0maccuracy_f1score_cal\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m\"../Morphological/DatasetV4/DatasetV4/test_masks1/test/*\"\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;34m\"../Morphological/weight/result/erode/*\"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 3\u001B[1;33m \u001B[0mprint\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34mf\"accuracy_erode: {acc_erode[0]:0.5f}\"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      4\u001B[0m \u001B[0mprint\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34mf\"f1_Score_erode: {acc_erode[1]:0.5f}\"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      5\u001B[0m \u001B[0mprint\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34mf\"IoU_erode: {acc_erode[2]:0.5f}\"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mIndexError\u001B[0m: invalid index to scalar variable."
     ]
    }
   ],
   "source": [
    "\n",
    "acc_erode = accuracy_f1score_cal(\"../Morphological/DatasetV4/DatasetV4/test_masks1/test/*\",\"../Morphological/weight/result/erode/*\")\n",
    "print(f\"accuracy_erode: {acc_erode[0]:0.5f}\")\n",
    "print(f\"f1_Score_erode: {acc_erode[1]:0.5f}\")\n",
    "print(f\"IoU_erode: {acc_erode[2]:0.5f}\")\n",
    "print(f\"pre_erode: {acc_erode[3]:0.5f}\")\n",
    "print(f\"recall_erode: {acc_erode[4]:0.5f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21/21 [00:04<00:00,  4.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy_dilated:0.97505\n",
      "f1_Score_dilated:0.93117\n",
      "IoU_dilated:0.87488\n",
      "pre_dilated:0.89974\n",
      "recall_dilated:0.97102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "acc_dilated = accuracy_f1score_cal(\"../Morphological/DatasetV4/DatasetV4/test_masks1/test/*\",\"../Morphological/weight/result/dilated/*\")\n",
    "print(f\"accuracy_dilated:{acc_dilated[0]:0.5f}\")\n",
    "print(f\"f1_Score_dilated:{acc_dilated[1]:0.5f}\")\n",
    "print(f\"IoU_dilated:{acc_dilated[2]:0.5f}\")\n",
    "print(f\"pre_dilated:{acc_dilated[3]:0.5f}\")\n",
    "print(f\"recall_dilated:{acc_dilated[4]:0.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21/21 [00:04<00:00,  4.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy_dilated_opening:0.98052\n",
      "f1_Score_dilated_opening:0.94717\n",
      "IoU_dilated_opening:0.90434\n",
      "pre_dilated_opening:0.93527\n",
      "recall_dilated_opening:0.96550\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "acc_dilated1 = accuracy_f1score_cal(\"../Morphological/DatasetV4/DatasetV4/test_masks1/test/*\",\"../Morphological/weight/result/dilated1/*\")\n",
    "print(f\"accuracy_dilated_opening:{acc_dilated1[0]:0.5f}\")\n",
    "print(f\"f1_Score_dilated_opening:{acc_dilated1[1]:0.5f}\")\n",
    "print(f\"IoU_dilated_opening:{acc_dilated1[2]:0.5f}\")\n",
    "print(f\"pre_dilated_opening:{acc_dilated1[3]:0.5f}\")\n",
    "print(f\"recall_dilated_opening:{acc_dilated1[4]:0.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no morph                         \n",
    "accuracy:0.95996\n",
    "f1_Score:0.94198            \n",
    "IoU:0.89187\n",
    "pre:0.91148\n",
    "recall:0.97645\n",
    "\n",
    "erode(10,10),25\n",
    "accuracy:0.96665\n",
    "f1_Score:0.95008\n",
    "IoU:0.90639\n",
    "pre:0.95387\n",
    "recall:0.94780\n",
    "\n",
    "erode(7,7),25\n",
    "accuracy:0.96665\n",
    "f1_Score:0.95008\n",
    "IoU:0.90639\n",
    "pre:0.95387\n",
    "recall:0.94780\n",
    "\n",
    "erode(7,7),45\n",
    "accuracy:0.96665\n",
    "f1_Score:0.95008\n",
    "IoU:0.90639\n",
    "pre:0.95387\n",
    "recall:0.94780\n",
    "\n",
    "erode(7,7),45 opening (4,4)\n",
    "accuracy:0.96511\n",
    "f1_Score:0.94753\n",
    "IoU:0.90181\n",
    "pre:0.95438\n",
    "recall:0.94223\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "erode \n",
    "(7,7),1\n",
    "accuracy_erode:0.96665\n",
    "f1_Score_erode:0.95008\n",
    "IoU_erode:0.90639\n",
    "pre_erode:0.95387\n",
    "recall_erode:0.94780\n",
    "\n",
    "(7,7),1\n",
    "accuracy_erode: 0.96665\n",
    "f1_Score_erode: 0.95008\n",
    "IoU_erode: 0.90639\n",
    "pre_erode: 0.95387\n",
    "recall_erode: 0.94780\n",
    "\n",
    "(4,4),1\n",
    "accuracy_erode: 0.96520\n",
    "f1_Score_erode: 0.94866\n",
    "IoU_erode: 0.90382\n",
    "pre_erode: 0.93526\n",
    "recall_erode: 0.96403\n",
    "\n",
    "(3,3),2\n",
    "accuracy_erode: 0.96653\n",
    "f1_Score_erode: 0.95044\n",
    "IoU_erode: 0.90703\n",
    "pre_erode: 0.94177\n",
    "recall_erode: 0.96080\n",
    "\n",
    "(8,8),1\n",
    "accuracy_erode: 0.96533\n",
    "f1_Score_erode: 0.94764\n",
    "IoU_erode: 0.90203\n",
    "pre_erode: 0.95975\n",
    "recall_erode: 0.93730\n",
    "\n",
    "(6,6),1\n",
    "accuracy_erode: 0.96619\n",
    "f1_Score_erode: 0.94958\n",
    "IoU_erode: 0.90548\n",
    "pre_erode: 0.94816\n",
    "recall_erode: 0.95249\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dilated\n",
    "(7,7),45\n",
    "accuracy_dilated:0.93916\n",
    "f1_Score_dilated:0.91551\n",
    "IoU_dilated:0.84623\n",
    "pre_dilated:0.85229\n",
    "recall_dilated:0.99177\n",
    "\n",
    "(10,10),10\n",
    "accuracy_dilated:0.93916\n",
    "f1_Score_dilated:0.91551\n",
    "IoU_dilated:0.84623\n",
    "pre_dilated:0.85229\n",
    "recall_dilated:0.99177\n",
    "\n",
    "(4,4)\n",
    "accuracy_dilated:0.95159\n",
    "f1_Score_dilated:0.93107\n",
    "IoU_dilated:0.87279\n",
    "pre_dilated:0.88443\n",
    "recall_dilated:0.98519\n",
    "\n",
    "(3,3)\n",
    "accuracy_dilated:0.95454\n",
    "f1_Score_dilated:0.93489\n",
    "IoU_dilated:0.87940\n",
    "pre_dilated:0.89364\n",
    "recall_dilated:0.98224\n",
    "\n",
    "(2,2)\n",
    "accuracy_dilated:0.95773\n",
    "f1_Score_dilated:0.93903\n",
    "IoU_dilated:0.88669\n",
    "pre_dilated:0.90351\n",
    "recall_dilated:0.97947\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "opening\n",
    "(4,4)\n",
    "accuracy_opening:0.96024\n",
    "f1_Score_opening:0.94220\n",
    "IoU_opening:0.89229\n",
    "pre_opening:0.91276\n",
    "recall_opening:0.97547\n",
    "\n",
    "(20,20)\n",
    "accuracy_opening:0.96168\n",
    "f1_Score_opening:0.94434\n",
    "IoU_opening:0.89599\n",
    "pre_opening:0.91919\n",
    "recall_opening:0.97264\n",
    "\n",
    "(3,3)\n",
    "accuracy_opening:0.95997\n",
    "f1_Score_opening:0.94201\n",
    "IoU_opening:0.89192\n",
    "pre_opening:0.91149\n",
    "recall_opening:0.97649\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "closing\n",
    "(7,7)\n",
    "accuracy_closing:0.95921\n",
    "f1_Score_closing:0.94111\n",
    "IoU_closing:0.89032\n",
    "pre_closingg:0.90798\n",
    "recall_closing:0.97861\n",
    "\n",
    "(4,4)\n",
    "accuracy_closing:0.96009\n",
    "f1_Score_closing:0.94201\n",
    "IoU_closing:0.89196\n",
    "pre_closingg:0.91178\n",
    "recall_closing:0.97620\n",
    "\n",
    "(3,3)\n",
    "accuracy_closing:0.95998\n",
    "f1_Score_closing:0.94203\n",
    "IoU_closing:0.89196\n",
    "pre_closingg:0.91084\n",
    "recall_closing:0.97728\n",
    "\n",
    "accuracy_closing:0.96021\n",
    "f1_Score_closing:0.94215\n",
    "IoU_closing:0.89221\n",
    "pre_closingg:0.91263\n",
    "recall_closing:0.97551"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dilated_opening\n",
    "(7,7),45 (4,4)\n",
    "accuracy_dilated_opening:0.93967\n",
    "f1_Score_dilated_opening:0.91591\n",
    "IoU_dilated_opening:0.84700\n",
    "pre_dilated_opening:0.85399\n",
    "recall_dilated_opening:0.99054\n",
    "\n",
    "(4,4),45 (4,4)\n",
    "accuracy_dilated_opening:0.95117\n",
    "f1_Score_dilated_opening:0.93040\n",
    "IoU_dilated_opening:0.87167\n",
    "pre_dilated_opening:0.88380\n",
    "recall_dilated_opening:0.98460\n",
    "\n",
    "(3,3),45 (3,3)\n",
    "accuracy_dilated_opening:0.95454\n",
    "f1_Score_dilated_opening:0.93489\n",
    "IoU_dilated_opening:0.87940\n",
    "pre_dilated_opening:0.89364\n",
    "recall_dilated_opening:0.98224\n",
    "\n",
    "(8,8),45 (2,2)\n",
    "accuracy_dilated_opening:0.96282\n",
    "f1_Score_dilated_opening:0.94357\n",
    "IoU_dilated_opening:0.89475\n",
    "pre_dilated_opening:0.95939\n",
    "recall_dilated_opening:0.92972\n",
    "\n",
    "(6,6),100 (2,2)\n",
    "accuracy_dilated_opening:0.96408\n",
    "f1_Score_dilated_opening:0.94616\n",
    "IoU_dilated_opening:0.89937\n",
    "pre_dilated_opening:0.94795\n",
    "recall_dilated_opening:0.94585\n",
    "\n",
    "(7,7),100 (2,2)\n",
    "accuracy_dilated_opening:0.96507\n",
    "f1_Score_dilated_opening:0.94746\n",
    "IoU_dilated_opening:0.90169\n",
    "pre_dilated_opening:0.95422\n",
    "recall_dilated_opening:0.94224"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "testPytorchModel",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 ('mytfenv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "vscode": {
   "interpreter": {
    "hash": "8ee986818394099986f64a36f8d70ae1b1a05ee74ece6160134b4f91f22ec808"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}